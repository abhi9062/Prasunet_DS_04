# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uUiZs0_k1OLPT7btROJrLYk5QuEnm01l
"""

import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Load the datasets
train_data = pd.read_csv('/content/twitter_training.csv')
validation_data = pd.read_csv('/content/twitter_validation.csv')

# Rename the columns for clarity
train_data.columns = ['ID', 'Topic', 'Sentiment', 'Tweet']
validation_data.columns = ['ID', 'Topic', 'Sentiment', 'Tweet']

# Combine the datasets
combined_data = pd.concat([train_data, validation_data], ignore_index=True)

# Function to clean and preprocess the text
def preprocess_text(text):
    if isinstance(text, str):
        text = text.lower()  # Convert to lowercase
        text = re.sub(r'\d+', '', text)  # Remove numbers
        text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
        text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    return text

# Apply the preprocessing function
combined_data['Cleaned_Tweet'] = combined_data['Tweet'].apply(preprocess_text)

# Plot the sentiment distribution
plt.figure(figsize=(10, 5))
sns.countplot(x='Sentiment', data=combined_data, order=combined_data['Sentiment'].value_counts().index)
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

# Generate word clouds for each sentiment category
sentiments = combined_data['Sentiment'].unique()

for sentiment in sentiments:
    sentiment_data = combined_data[combined_data['Sentiment'] == sentiment]
    text = ' '.join(sentiment_data['Cleaned_Tweet'].dropna())
    wordcloud = WordCloud(width=800, height=400, max_words=100).generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Word Cloud for {sentiment} Sentiment')
    plt.axis('off')
    plt.show()

# Time Series Analysis (assuming a 'Timestamp' column exists)
if 'Timestamp' in combined_data.columns:
    combined_data['Timestamp'] = pd.to_datetime(combined_data['Timestamp'])
    combined_data.set_index('Timestamp', inplace=True)
    combined_data['Sentiment'].resample('M').value_counts().unstack().plot(kind='line', figsize=(15, 7))
    plt.title('Sentiment Over Time')
    plt.xlabel('Time')
    plt.ylabel('Count')
    plt.show()

# Topic Modeling
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
dtm = vectorizer.fit_transform(combined_data['Cleaned_Tweet'].dropna())
lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda.fit(dtm)

# Display the top words in each topic
for index, topic in enumerate(lda.components_):
    print(f'Top 10 words for Topic #{index}')
    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])
    print('\n')